{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##1. Explain the properties of the F-distribution."
      ],
      "metadata": {
        "id": "a07lzmHGFFHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution in statistics has these properties:\n",
        "\n",
        "1. **Non-Negative**: Since it’s a ratio of variances, it only takes positive values.\n",
        "  \n",
        "2. **Right-Skewed Shape**: It's skewed to the right, especially for smaller sample sizes. The skew decreases with larger sample sizes.\n",
        "\n",
        "3. **Degrees of Freedom**: Defined by two degrees of freedom: \\(df_1\\) (numerator) and \\(df_2\\) (denominator), which shape the distribution.\n",
        "\n",
        "4. **Mean and Variance**:\n",
        "   - Mean is \\$\frac{df_2}{df_2 - 2}\\) (when \\(df_2 > 2\\)$).\n",
        "   - Variance depends on both $\\(df_1\\) and \\(df_2\\), requiring \\(df_2 > 4\\)$.\n",
        "\n",
        "5. **Application**: Used to test hypotheses about the equality of variances in ANOVA, regression analysis, and comparing two variances."
      ],
      "metadata": {
        "id": "vVRTFEIkFIvP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
      ],
      "metadata": {
        "id": "2dAYXr94FQo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The F-distribution is used in these key statistical tests:\n",
        "\n",
        "1. **ANOVA (Analysis of Variance)**: It compares the variances between groups to determine if there are significant differences between group means. The F-distribution is appropriate because it models the ratio of variances, helping test if group variance differs significantly from within-group variance.\n",
        "\n",
        "2. **Regression Analysis**: In multiple regression, the F-test assesses the overall significance of the model by comparing explained and unexplained variances. It’s appropriate because it indicates if the predictor variables collectively explain a significant amount of the variance in the outcome.\n",
        "\n",
        "3. **Comparing Two Variances**: The F-test checks if two populations have equal variances. This is fitting as the F-distribution models the ratio of variances directly, making it ideal for testing variance equality.\n",
        "\n",
        "The F-distribution suits these tests as it is specifically designed to assess the ratio of variances, making it useful for comparing variability in various contexts."
      ],
      "metadata": {
        "id": "2qDaKoSxFx0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?"
      ],
      "metadata": {
        "id": "3rp_HYLmF2lC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key assumptions for conducting an F-test to compare variances of two populations are:\n",
        "\n",
        "1. **Normality**: Both populations should be normally distributed. The F-test is sensitive to deviations from normality, which can affect its accuracy.\n",
        "\n",
        "2. **Independence**: The samples from each population must be independent of each other. This means that the data in one sample should not influence the data in the other.\n",
        "\n",
        "3. **Random Sampling**: The data should be collected through random sampling to ensure it accurately represents the populations.\n",
        "\n",
        "These assumptions help ensure the validity of the F-test results when comparing population variances."
      ],
      "metadata": {
        "id": "iA3Yitp4GEym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. What is the purpose of ANOVA, and how does it differ from a t-test?"
      ],
      "metadata": {
        "id": "pfim3IY6GQmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of **ANOVA (Analysis of Variance)** is to test if there are significant differences between the means of three or more groups. It assesses whether any group mean differs from others by comparing within-group and between-group variance.\n",
        "\n",
        "**Differences from a t-test**:\n",
        "- **Group Comparison**: A t-test compares means between *two* groups, while ANOVA can compare *three or more* groups simultaneously.\n",
        "- **Error Rate**: ANOVA avoids increasing the Type I error rate, which happens when multiple t-tests are used to compare more than two groups.\n",
        "\n",
        "In short, ANOVA is more suitable than a t-test when comparing multiple groups, providing a single test to identify significant mean differences without inflated error rates."
      ],
      "metadata": {
        "id": "L2miho19GXbo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups."
      ],
      "metadata": {
        "id": "RpfkLKQ-GwvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You would use a **one-way ANOVA** instead of multiple t-tests when comparing more than two groups to control the **Type I error rate** (false positives). Here’s why:\n",
        "\n",
        "1. **Error Control**: Each t-test has a certain probability of a Type I error (usually 5%). Conducting multiple t-tests increases the cumulative error rate, which increases the chance of falsely finding a significant result. One-way ANOVA, however, performs a single test, keeping the error rate at the desired level.\n",
        "\n",
        "2. **Efficiency**: ANOVA allows for a single comparison across all groups, making it faster and simpler to interpret when analyzing three or more groups.\n",
        "\n",
        "3. **Overall Significance Testing**: One-way ANOVA identifies if any group differs from the others as a whole. If significant, you can then perform post hoc tests to determine which specific groups differ."
      ],
      "metadata": {
        "id": "UWZHeccgG3YF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?"
      ],
      "metadata": {
        "id": "PhOitOA4HHDu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In **ANOVA**, variance is partitioned into:\n",
        "\n",
        "1. **Between-Group Variance**: Measures the variance due to differences between the group means. It reflects how much each group's mean deviates from the overall mean, indicating the effect of the independent variable.\n",
        "\n",
        "2. **Within-Group Variance**: Measures the variance within each group, capturing random fluctuations or individual differences within each group. It represents the variability not explained by the group effect.\n",
        "\n",
        "### Contribution to the F-Statistic:\n",
        "The **F-statistic** in ANOVA is calculated as the ratio of between-group variance to within-group variance:\n",
        "\n",
        "$$\n",
        "F = \\frac{\\text{Between-Group Variance}}{\\text{Within-Group Variance}}\n",
        "$$\n",
        "\n",
        "- A larger between-group variance compared to within-group variance results in a higher F-statistic, suggesting that group means differ significantly.\n",
        "- If the F-statistic is large enough (based on an F-distribution), it indicates that at least one group mean significantly differs from others.\n",
        "\n",
        "This partitioning helps determine if observed differences are likely due to the treatment effect or random variation within groups."
      ],
      "metadata": {
        "id": "XbKqIK9XHc4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
      ],
      "metadata": {
        "id": "njxeCxE4HumS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **classical (frequentist) ANOVA** and **Bayesian ANOVA** approaches differ significantly in handling uncertainty, parameter estimation, and hypothesis testing:\n",
        "\n",
        "### 1. **Handling Uncertainty**:\n",
        "   - **Frequentist ANOVA**: Uses p-values to test the null hypothesis, estimating the likelihood of observed results assuming the null hypothesis is true. Uncertainty is expressed as a probability of observing the data under repeated sampling.\n",
        "   - **Bayesian ANOVA**: Provides probability distributions for parameters, capturing uncertainty in a more direct, probabilistic form. It estimates the probability of hypotheses given the data, which can lead to a more intuitive understanding of uncertainty.\n",
        "\n",
        "### 2. **Parameter Estimation**:\n",
        "   - **Frequentist ANOVA**: Estimates parameters (e.g., group means) through point estimates, relying on sample data to calculate averages and variances. Confidence intervals give an estimated range for these parameters.\n",
        "   - **Bayesian ANOVA**: Estimates parameters as probability distributions (posterior distributions) rather than single values, incorporating prior knowledge or beliefs about the parameters. This allows for a richer and often more flexible view of parameter uncertainty.\n",
        "\n",
        "### 3. **Hypothesis Testing**:\n",
        "   - **Frequentist ANOVA**: Tests hypotheses using p-values, which indicate the probability of the observed (or more extreme) data given the null hypothesis. A low p-value suggests rejecting the null hypothesis.\n",
        "   - **Bayesian ANOVA**: Tests hypotheses by comparing posterior probabilities, providing direct probabilities for competing hypotheses (e.g., Bayesian Factor). This lets researchers assess the relative evidence for each hypothesis and update beliefs as more data is collected.\n",
        "\n",
        "### Summary\n",
        "Frequentist ANOVA relies on p-values and point estimates, focusing on repeated sampling interpretation, while Bayesian ANOVA uses probability distributions for parameters, enabling direct probability statements about hypotheses. Bayesian ANOVA can be more flexible but requires specifying prior distributions, which can be subjective.The **classical (frequentist) ANOVA** and **Bayesian ANOVA** approaches differ significantly in handling uncertainty, parameter estimation, and hypothesis testing:\n",
        "\n",
        "### 1. **Handling Uncertainty**:\n",
        "   - **Frequentist ANOVA**: Uses p-values to test the null hypothesis, estimating the likelihood of observed results assuming the null hypothesis is true. Uncertainty is expressed as a probability of observing the data under repeated sampling.\n",
        "   - **Bayesian ANOVA**: Provides probability distributions for parameters, capturing uncertainty in a more direct, probabilistic form. It estimates the probability of hypotheses given the data, which can lead to a more intuitive understanding of uncertainty.\n",
        "\n",
        "### 2. **Parameter Estimation**:\n",
        "   - **Frequentist ANOVA**: Estimates parameters (e.g., group means) through point estimates, relying on sample data to calculate averages and variances. Confidence intervals give an estimated range for these parameters.\n",
        "   - **Bayesian ANOVA**: Estimates parameters as probability distributions (posterior distributions) rather than single values, incorporating prior knowledge or beliefs about the parameters. This allows for a richer and often more flexible view of parameter uncertainty.\n",
        "\n",
        "### 3. **Hypothesis Testing**:\n",
        "   - **Frequentist ANOVA**: Tests hypotheses using p-values, which indicate the probability of the observed (or more extreme) data given the null hypothesis. A low p-value suggests rejecting the null hypothesis.\n",
        "   - **Bayesian ANOVA**: Tests hypotheses by comparing posterior probabilities, providing direct probabilities for competing hypotheses (e.g., Bayesian Factor). This lets researchers assess the relative evidence for each hypothesis and update beliefs as more data is collected.\n",
        "\n",
        "### Summary\n",
        "Frequentist ANOVA relies on p-values and point estimates, focusing on repeated sampling interpretation, while Bayesian ANOVA uses probability distributions for parameters, enabling direct probability statements about hypotheses. Bayesian ANOVA can be more flexible but requires specifying prior distributions, which can be subjective."
      ],
      "metadata": {
        "id": "faimEaprH6Cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Question: You have two sets of data representing the incomes of two different professions"
      ],
      "metadata": {
        "id": "ke246ZPHJKBM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ##**Profession A:** [48, 52, 55, 60, 62]\n",
        " ##**Profession B:** [45, 50, 55, 52, 47]\n",
        " ##Perform an F-test to determine if the variances of the two professions' incomes are equal. What are your conclusions based on the F-test?\n",
        " ##**Task:** Use Python to calculate the F-statistic and p-value for the given data.\n",
        " ##**Objective:** Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "wYzQZ-k_IML1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Data for the two professions\n",
        "profession_A = [48, 52, 55, 60, 62]\n",
        "profession_B = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Calculate the variances of the two samples\n",
        "var_A = np.var(profession_A, ddof=1)  # Sample variance\n",
        "var_B = np.var(profession_B, ddof=1)  # Sample variance\n",
        "\n",
        "# Calculate the F-statistic\n",
        "F_statistic = var_A / var_B\n",
        "\n",
        "# Calculate the degrees of freedom for each sample\n",
        "df_A = len(profession_A) - 1\n",
        "df_B = len(profession_B) - 1\n",
        "\n",
        "# Calculate the p-value using the F-distribution\n",
        "p_value = stats.f.sf(F_statistic, df_A, df_B) * 2  # Two-tailed test\n",
        "\n",
        "F_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8MpMxDuJ6ba",
        "outputId": "1e8a90db-da12-4740-d7c4-8a2bf9669544"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2.089171974522293, 0.4930485990053393)"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **F-statistic** for comparing the variances of the two professions' incomes is approximately **2.09**, with a **p-value of 0.493**.\n",
        "\n",
        "### Interpretation:\n",
        "Since the p-value (0.493) is much greater than the typical significance level (e.g., 0.05), we **do not reject the null hypothesis**. This indicates that there is no significant evidence to suggest that the variances of incomes in the two professions are different."
      ],
      "metadata": {
        "id": "_8BB-QnyI3VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data:\n",
        "## Region A: [160, 162, 165, 158, 164]\n",
        "## Region B: [172, 175, 170, 168, 174]\n",
        "## Region C: [180, 182, 179, 185, 183]\n",
        "## Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "## Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value."
      ],
      "metadata": {
        "id": "PW5WJtVZKLbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Data for the three regions\n",
        "region_A = [160, 162, 165, 158, 164]\n",
        "region_B = [172, 175, 170, 168, 174]\n",
        "region_C = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "F_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
        "\n",
        "F_statistic, p_value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfA8PDIBKz0I",
        "outputId": "17202a89-ed9a-4c77-f1e3-6f3321c0e4a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(67.87330316742101, 2.870664187937026e-07)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The results of the one-way ANOVA are as follows:\n",
        "\n",
        "- **F-statistic**: approximately **67.87**\n",
        "- **p-value**: approximately **2.87 × 10⁻⁷**\n",
        "\n",
        "### Interpretation:\n",
        "Since the p-value is significantly less than the common significance level (e.g., 0.05), we **reject the null hypothesis**. This indicates that there are statistically significant differences in average heights between the three regions.\n",
        "\n",
        "### Conclusion:\n",
        "At least one region's average height differs from the others, suggesting that the heights in Region A, Region B, and Region C are not all the same. Further post hoc tests can identify which specific regions differ."
      ],
      "metadata": {
        "id": "eHnZhSoLKp1-"
      }
    }
  ]
}
